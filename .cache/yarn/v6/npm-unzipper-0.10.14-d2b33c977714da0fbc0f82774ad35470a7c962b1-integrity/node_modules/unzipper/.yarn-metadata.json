{
  "manifest": {
    "name": "unzipper",
    "version": "0.10.14",
    "description": "Unzip cross-platform streaming API ",
    "author": {
      "name": "Evan Oxfeld",
      "email": "eoxfeld@gmail.com"
    },
    "contributors": [
      {
        "name": "Ziggy Jonsson",
        "email": "ziggy.jonsson.nyc@gmail.com"
      },
      {
        "name": "Evan Oxfeld",
        "email": "eoxfeld@gmail.com"
      },
      {
        "name": "Joe Ferner",
        "email": "joe.ferner@nearinfinity.com"
      }
    ],
    "repository": {
      "type": "git",
      "url": "https://github.com/ZJONSSON/node-unzipper.git"
    },
    "license": "MIT",
    "dependencies": {
      "big-integer": "^1.6.17",
      "binary": "~0.3.0",
      "bluebird": "~3.4.1",
      "buffer-indexof-polyfill": "~1.0.0",
      "duplexer2": "~0.1.4",
      "fstream": "^1.0.12",
      "graceful-fs": "^4.2.2",
      "listenercount": "~1.0.1",
      "readable-stream": "~2.3.6",
      "setimmediate": "~1.0.4"
    },
    "devDependencies": {
      "aws-sdk": "^2.77.0",
      "dirdiff": ">= 0.0.1 < 1",
      "iconv-lite": "^0.4.24",
      "request": "^2.88.0",
      "stream-buffers": ">= 0.2.5 < 1",
      "tap": ">= 0.3.0 < 1",
      "temp": ">= 0.4.0 < 1"
    },
    "keywords": [
      "zip",
      "unzip",
      "zlib",
      "uncompress",
      "archive",
      "stream",
      "extract"
    ],
    "main": "unzip.js",
    "scripts": {
      "test": "tap test/*.js --jobs=10 --coverage-report=html --no-browser"
    },
    "_registry": "npm",
    "_loc": "/home/dpcsdev/.cache/yarn/v6/npm-unzipper-0.10.14-d2b33c977714da0fbc0f82774ad35470a7c962b1-integrity/node_modules/unzipper/package.json",
    "readmeFilename": "README.md",
    "readme": "[![NPM Version][npm-image]][npm-url]\n[![NPM Downloads][downloads-image]][downloads-url]\n[![Test Coverage][travis-image]][travis-url]\n[![Coverage][coverage-image]][coverage-url]\n\n[npm-image]: https://img.shields.io/npm/v/unzipper.svg\n[npm-url]: https://npmjs.org/package/unzipper\n[travis-image]: https://api.travis-ci.org/ZJONSSON/node-unzipper.png?branch=master\n[travis-url]: https://travis-ci.org/ZJONSSON/node-unzipper?branch=master\n[downloads-image]: https://img.shields.io/npm/dm/unzipper.svg\n[downloads-url]: https://npmjs.org/package/unzipper\n[coverage-image]: https://3tjjj5abqi.execute-api.us-east-1.amazonaws.com/prod/node-unzipper/badge\n[coverage-url]: https://3tjjj5abqi.execute-api.us-east-1.amazonaws.com/prod/node-unzipper/url\n\n# unzipper\n\nThis is an active fork and drop-in replacement of the [node-unzip](https://github.com/EvanOxfeld/node-unzip) and addresses the following issues:\n* finish/close events are not always triggered, particular when the input stream is slower than the receivers\n* Any files are buffered into memory before passing on to entry\n\nThe structure of this fork is similar to the original, but uses Promises and inherit guarantees provided by node streams to ensure low memory footprint and emits finish/close events at the end of processing.   The new `Parser` will push any parsed `entries` downstream if you pipe from it, while still supporting the legacy `entry` event as well.\n\nBreaking changes: The new `Parser` will not automatically drain entries if there are no listeners or pipes in place.\n\nUnzipper provides simple APIs similar to [node-tar](https://github.com/isaacs/node-tar) for parsing and extracting zip files.\nThere are no added compiled dependencies - inflation is handled by node.js's built in zlib support.\n\nPlease note:  Methods that use the Central Directory instead of parsing entire file can be found under [`Open`](#open)\n\nChrome extension files (.crx) are zipfiles with an [extra header](http://www.adambarth.com/experimental/crx/docs/crx.html) at the start of the file.  Unzipper will parse .crx file with the streaming methods (`Parse` and `ParseOne`).  The `Open` methods will check for `crx` headers and parse crx files, but only if you provide `crx: true` in options.\n\n## Installation\n\n```bash\n$ npm install unzipper\n```\n\n## Quick Examples\n\n### Extract to a directory\n```js\nfs.createReadStream('path/to/archive.zip')\n  .pipe(unzipper.Extract({ path: 'output/path' }));\n```\n\nExtract emits the 'close' event once the zip's contents have been fully extracted to disk. `Extract` uses [fstream.Writer](https://www.npmjs.com/package/fstream) and therefore needs need an absolute path to the destination directory.  This directory will be automatically created if it doesn't already exits.\n\n### Parse zip file contents\n\nProcess each zip file entry or pipe entries to another stream.\n\n__Important__: If you do not intend to consume an entry stream's raw data, call autodrain() to dispose of the entry's\ncontents. Otherwise the stream will halt.   `.autodrain()` returns an empty stream that provides `error` and `finish` events.\nAdditionally you can call `.autodrain().promise()` to get the promisified version of success or failure of the autodrain.\n\n```js\n// If you want to handle autodrain errors you can either:\nentry.autodrain().catch(e => handleError);\n// or\nentry.autodrain().on('error' => handleError);\n```\n\nHere is a quick example:\n\n```js\nfs.createReadStream('path/to/archive.zip')\n  .pipe(unzipper.Parse())\n  .on('entry', function (entry) {\n    const fileName = entry.path;\n    const type = entry.type; // 'Directory' or 'File'\n    const size = entry.vars.uncompressedSize; // There is also compressedSize;\n    if (fileName === \"this IS the file I'm looking for\") {\n      entry.pipe(fs.createWriteStream('output/path'));\n    } else {\n      entry.autodrain();\n    }\n  });\n```\n\nand the same example using async iterators:\n\n```js\nconst zip = fs.createReadStream('path/to/archive.zip').pipe(unzipper.Parse({forceStream: true}));\nfor await (const entry of zip) {\n  const fileName = entry.path;\n  const type = entry.type; // 'Directory' or 'File'\n  const size = entry.vars.uncompressedSize; // There is also compressedSize;\n  if (fileName === \"this IS the file I'm looking for\") {\n    entry.pipe(fs.createWriteStream('output/path'));\n  } else {\n    entry.autodrain();\n  }\n}\n```\n\n### Parse zip by piping entries downstream\n\nIf you `pipe` from unzipper the downstream components will receive each `entry` for further processing.   This allows for clean pipelines transforming zipfiles into unzipped data.\n\nExample using `stream.Transform`:\n\n```js\nfs.createReadStream('path/to/archive.zip')\n  .pipe(unzipper.Parse())\n  .pipe(stream.Transform({\n    objectMode: true,\n    transform: function(entry,e,cb) {\n      const fileName = entry.path;\n      const type = entry.type; // 'Directory' or 'File'\n      const size = entry.vars.uncompressedSize; // There is also compressedSize;\n      if (fileName === \"this IS the file I'm looking for\") {\n        entry.pipe(fs.createWriteStream('output/path'))\n          .on('finish',cb);\n      } else {\n        entry.autodrain();\n        cb();\n      }\n    }\n  }\n  }));\n```\n\nExample using [etl](https://www.npmjs.com/package/etl):\n\n```js\nfs.createReadStream('path/to/archive.zip')\n  .pipe(unzipper.Parse())\n  .pipe(etl.map(entry => {\n    if (entry.path == \"this IS the file I'm looking for\")\n      return entry\n        .pipe(etl.toFile('output/path'))\n        .promise();\n    else\n      entry.autodrain();\n  }))\n\n```\n\n### Parse a single file and pipe contents\n\n`unzipper.parseOne([regex])` is a convenience method that unzips only one file from the archive and pipes the contents down (not the entry itself).  If no search criteria is specified, the first file in the archive will be unzipped.  Otherwise, each filename will be compared to the criteria and the first one to match will be unzipped and piped down.  If no file matches then the the stream will end without any content.\n\nExample:\n\n```js\nfs.createReadStream('path/to/archive.zip')\n  .pipe(unzipper.ParseOne())\n  .pipe(fs.createWriteStream('firstFile.txt'));\n```\n\n### Buffering the content of an entry into memory\n\nWhile the recommended strategy of consuming the unzipped contents is using streams, it is sometimes convenient to be able to get the full buffered contents of each file .  Each `entry` provides a `.buffer` function that consumes the entry by buffering the contents into memory and returning a promise to the complete buffer.\n\n```js\nfs.createReadStream('path/to/archive.zip')\n  .pipe(unzipper.Parse())\n  .pipe(etl.map(async entry => {\n    if (entry.path == \"this IS the file I'm looking for\") {\n      const content = await entry.buffer();\n      await fs.writeFile('output/path',content);\n    }\n    else {\n      entry.autodrain();\n    }\n  }))\n```\n\n### Parse.promise() syntax sugar\n\nThe parser emits `finish` and `error` events like any other stream.  The parser additionally provides a promise wrapper around those two events to allow easy folding into existing Promise-based structures.\n\nExample:\n\n```js\nfs.createReadStream('path/to/archive.zip')\n  .pipe(unzipper.Parse())\n  .on('entry', entry => entry.autodrain())\n  .promise()\n  .then( () => console.log('done'), e => console.log('error',e));\n```\n\n### Parse zip created by DOS ZIP or Windows ZIP Folders\n\nArchives created by legacy tools usually have filenames encoded with IBM PC (Windows OEM) character set.\nYou can decode filenames with preferred character set:\n\n```js\nconst il = require('iconv-lite');\nfs.createReadStream('path/to/archive.zip')\n  .pipe(unzipper.Parse())\n  .on('entry', function (entry) {\n    // if some legacy zip tool follow ZIP spec then this flag will be set\n    const isUnicode = entry.props.flags.isUnicode;\n    // decode \"non-unicode\" filename from OEM Cyrillic character set\n    const fileName = isUnicode ? entry.path : il.decode(entry.props.pathBuffer, 'cp866');\n    const type = entry.type; // 'Directory' or 'File'\n    const size = entry.vars.uncompressedSize; // There is also compressedSize;\n    if (fileName === \"Текстовый файл.txt\") {\n      entry.pipe(fs.createWriteStream(fileName));\n    } else {\n      entry.autodrain();\n    }\n  });\n```\n\n## Open\nPrevious methods rely on the entire zipfile being received through a pipe.  The Open methods load take a different approach: load the central directory first (at the end of the zipfile) and provide the ability to pick and choose which zipfiles to extract, even extracting them in parallel.   The open methods return a promise on the contents of the directory, with individual `files` listed in an array.   Each file element has the following methods:\n* `stream([password])` - returns a stream of the unzipped content which can be piped to any destination\n* `buffer([password])` - returns a promise on the buffered content of the file)\nIf the file is encrypted you will have to supply a password to decrypt, otherwise you can leave blank.\nUnlike `adm-zip` the Open methods will never read the entire zipfile into buffer.\n\nThe last argument is optional `options` object where you can specify `tailSize` (default 80 bytes), i.e. how many bytes should we read at the end of the zipfile to locate the endOfCentralDirectory.  This location can be variable depending on zip64 extensible data sector size.   Additionally you can supply option `crx: true` which will check for a crx header and parse the file accordingly by shifting all file offsets by the length of the crx header.\n\n### Open.file([path], [options])\nReturns a Promise to the central directory information with methods to extract individual files.   `start` and `end` options are used to avoid reading the whole file.\n\nExample:\n```js\nasync function main() {\n  const directory = await unzipper.Open.file('path/to/archive.zip');\n  console.log('directory', directory);\n  return new Promise( (resolve, reject) => {\n    directory.files[0]\n      .stream()\n      .pipe(fs.createWriteStream('firstFile'))\n      .on('error',reject)\n      .on('finish',resolve)\n  });\n}\n\nmain();\n```\n\n### Open.url([requestLibrary], [url | params], [options])\nThis function will return a Promise to the central directory information from a URL point to a zipfile.  Range-headers are used to avoid reading the whole file. Unzipper does not ship with a request library so you will have to provide it as the first option.\n\nLive Example: (extracts a tiny xml file from the middle of a 500MB zipfile)\n\n```js\nconst request = require('request');\nconst unzipper = require('./unzip');\n\nasync function main() {\n  const directory = await unzipper.Open.url(request,'http://www2.census.gov/geo/tiger/TIGER2015/ZCTA5/tl_2015_us_zcta510.zip');\n  const file = directory.files.find(d => d.path === 'tl_2015_us_zcta510.shp.iso.xml');\n  const content = await file.buffer();\n  console.log(content.toString());\n}\n\nmain();\n```\n\n\nThis function takes a second parameter which can either be a string containing the `url` to request, or an `options` object to invoke the supplied `request` library with. This can be used when other request options are required, such as custom headers or authentication to a third party service.\n\n```js\nconst request = require('google-oauth-jwt').requestWithJWT();\n\nconst googleStorageOptions = {\n  url: `https://www.googleapis.com/storage/v1/b/m-bucket-name/o/my-object-name`,\n  qs: { alt: 'media' },\n  jwt: {\n      email: google.storage.credentials.client_email,\n      key: google.storage.credentials.private_key,\n      scopes: ['https://www.googleapis.com/auth/devstorage.read_only']\n  }\n});\n\nasync function getFile(req, res, next) {\n  const directory = await unzipper.Open.url(request, googleStorageOptions);\n  const file = zip.files.find((file) => file.path === 'my-filename');\n  return file.stream().pipe(res);\n});\n```\n\n### Open.s3([aws-sdk], [params], [options])\nThis function will return a Promise to the central directory information from a zipfile on S3.  Range-headers are used to avoid reading the whole file.    Unzipper does not ship with with the aws-sdk so you have to provide an instantiated client as first arguments.    The params object requires `Bucket` and `Key` to fetch the correct file.\n\nExample:\n\n```js\nconst unzipper = require('./unzip');\nconst AWS = require('aws-sdk');\nconst s3Client = AWS.S3(config);\n\nasync function main() {\n  const directory = await unzipper.Open.s3(s3Client,{Bucket: 'unzipper', Key: 'archive.zip'});\n  return new Promise( (resolve, reject) => {\n    directory.files[0]\n      .stream()\n      .pipe(fs.createWriteStream('firstFile'))\n      .on('error',reject)\n      .on('finish',resolve)\n  });\n}\n\nmain();\n```\n\n### Open.buffer(buffer, [options])\nIf you already have the zip file in-memory as a buffer, you can open the contents directly.\n\nExample:\n\n```js\n// never use readFileSync - only used here to simplify the example\nconst buffer = fs.readFileSync('path/to/arhive.zip');\n\nasync function main() {\n  const directory = await unzipper.Open.buffer(buffer);\n  console.log('directory',directory);\n  // ...\n}\n\nmain();\n```\n\n### Open.custom(source, [options])\nThis function can be used to provide a custom source implementation. The source parameter expects a `stream` and a `size` function to be implemented. The size function should return a `Promise` that resolves the total size of the file. The stream function should return a `Readable` stream according to the supplied offset and length parameters.\n\nExample:\n\n```js\n// Custom source implementation for reading a zip file from Google Cloud Storage\nconst { Storage } = require('@google-cloud/storage');\n\nasync function main() {\n  const storage = new Storage();\n  const bucket = storage.bucket('my-bucket');\n  const zipFile = bucket.file('my-zip-file.zip');\n  \n  const customSource = {\n    stream: function(offset, length) {\n      return zipFile.createReadStream({\n        start: offset,\n        end: length && offset + length\n      })\n    },\n    size: async function() {\n      const objMetadata = (await zipFile.getMetadata())[0];\n      return objMetadata.size;\n    }\n  };\n\n  const directory = await unzipper.Open.custom(customSource);\n  console.log('directory', directory);\n  // ...\n}\n\nmain();\n```\n\n### Open.[method].extract()\n\nThe directory object returned from `Open.[method]` provides an `extract` method which extracts all the files to a specified `path`, with an optional `concurrency` (default: 1).\n\nExample (with concurrency of 5):\n\n```js\nunzip.Open.file('path/to/archive.zip')\n  .then(d => d.extract({path: '/extraction/path', concurrency: 5}));\n```\n\n## Licenses\nSee LICENCE\n",
    "licenseText": "Copyright (c) 2012 - 2013 Near Infinity Corporation\n\nPermission is hereby granted, free of charge, to any person obtaining\na copy of this software and associated documentation files (the\n\"Software\"), to deal in the Software without restriction, including\nwithout limitation the rights to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so, subject to\nthe following conditions:\n\nThe above copyright notice and this permission notice shall be\nincluded in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\nNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\nLIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\nWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n---\n\nCommits in this fork are (c) Ziggy Jonsson (ziggy.jonsson.nyc@gmail.com)\nand fall under same licence structure as the original repo (MIT)"
  },
  "artifacts": [],
  "remote": {
    "resolved": "https://registry.yarnpkg.com/unzipper/-/unzipper-0.10.14.tgz#d2b33c977714da0fbc0f82774ad35470a7c962b1",
    "type": "tarball",
    "reference": "https://registry.yarnpkg.com/unzipper/-/unzipper-0.10.14.tgz",
    "hash": "d2b33c977714da0fbc0f82774ad35470a7c962b1",
    "integrity": "sha512-ti4wZj+0bQTiX2KmKWuwj7lhV+2n//uXEotUmGuQqrbVZSEGFMbI68+c6JCQ8aAmUWYvtHEz2A8K6wXvueR/6g==",
    "registry": "npm",
    "packageName": "unzipper",
    "cacheIntegrity": "sha512-ti4wZj+0bQTiX2KmKWuwj7lhV+2n//uXEotUmGuQqrbVZSEGFMbI68+c6JCQ8aAmUWYvtHEz2A8K6wXvueR/6g== sha1-0rM8l3cU2g+8D4J3StNUcKfJYrE="
  },
  "registry": "npm",
  "hash": "d2b33c977714da0fbc0f82774ad35470a7c962b1"
}